{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3WUfjSGxQOg"
      },
      "source": [
        "# Part 0: Argument and Import Initialization\n",
        "\n",
        "- The \"Args\" class is initialized to contain any pre-defined hyperparameters and variables used in the experiment\n",
        "- Imports consists of three sections. The fist part titled \"Google Colab Imports\" is optional and is only used to extract a zip file if the data is loaded as a zip file into google drive - the reason this is done is to unzip the data and load it into the coloab notebooks local memory for efficiency reasons. Otherwise, the data might be read in from the drive image by image during training, which greatly reduces training efficiency. This section can take around 10 minutes to complete.\n",
        "- Basic torch, pandas, and numpy imports follow, as well as modules necessary to read in and modify the images from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 GOOGLE COLAB - DATASET IMPORTS\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import threading\n",
        "from google.colab import drive, files\n",
        "\n",
        "# 2 BASIC TORCH IMPORTS\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from  torchtext.data.utils import get_tokenizer\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "import torchtext as tt\n",
        "import collections\n",
        "\n",
        "# 3 READING IN IMAGES FROM DATASET\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "\n",
        "# 4 TRANSFORMING IMAGES TO DATA\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 5 BUILDING VOCABULARY FROM CAPTIONS\n",
        "from torchvision.datasets import CocoCaptions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "from torchtext.vocab import vocab"
      ],
      "metadata": {
        "id": "AH6akud3DTMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KlbIDoAGw_C3"
      },
      "outputs": [],
      "source": [
        "# 6 INITIALIZE ARGUMENTS\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.embedding_dim = 300\n",
        "        self.hidden_size = 512\n",
        "        self.max_caption_length = 20\n",
        "\n",
        "        self.batch_size = 128\n",
        "        self.epochs = 30\n",
        "        self.lr = 0.001\n",
        "\n",
        "        self.side_length = 224\n",
        "\n",
        "        self.train_img_data_path = './data/train2017'\n",
        "        self.val_img_data_path = './data/val2017'\n",
        "\n",
        "        self.train_ann_path = './data/annotations_trainval2017/captions_train2017.json'\n",
        "        self.val_ann_path = './data/annotations_trainval2017/captions_val2017.json'\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfD6qZx9xtDq"
      },
      "outputs": [],
      "source": [
        "# 7 GOOGLE COLAB + DATASET PREPARATION\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_urls = [\n",
        "    \"http://images.cocodataset.org/zips/train2017.zip\",\n",
        "    \"http://images.cocodataset.org/zips/val2017.zip\",\n",
        "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "]\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/data/'\n",
        "extract_dir = './data'\n",
        "\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "start_time = time.time()\n",
        "for url in dataset_urls:\n",
        "    filename = os.path.join(save_dir, url.split('/')[-1])\n",
        "\n",
        "    if not os.path.exists(filename):\n",
        "        !wget {url} -P {save_dir}\n",
        "\n",
        "    !unzip -q -u {filename} -d {extract_dir}\n",
        "\n",
        "end_time = time.time()\n",
        "time_taken = end_time - start_time\n",
        "print(\"Time Taken:\", time_taken / 60, \"minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQKgIt6gy7UD"
      },
      "source": [
        "# Part 1: Dataset Initilization\n",
        "\n",
        "- Definite the IMAGE_DATASET class with needed functions. Each image is resized to 64 x 64 pixels, and expanded to 3 channels to ensure consistency in training. The COCO module is used to associate images and image annotations in the COCO dataset.\n",
        "- Training and Test datasets are initialized from this class as divided originally in the dataset and a DataLoader is initialized\n",
        "- A vocabulary is also initialized based on the image captions loaded in from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n95RUTt8wk3I"
      },
      "outputs": [],
      "source": [
        "class IMAGE_DATASET(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.coco = COCO(annotations_file)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.img_keys = list(self.coco.imgs.keys())\n",
        "\n",
        "    def get_single_image(self, img_id):\n",
        "        # load image and annotations\n",
        "        img = self.coco.loadImgs(img_id)[0]\n",
        "        annotation_ids = self.coco.getAnnIds(img_id)\n",
        "        annotations = self.coco.loadAnns(annotation_ids)\n",
        "\n",
        "        # open and transform image to tensor with needed dimensions\n",
        "        path = os.path.join(self.img_dir, img['file_name'])\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption = random.choice(annotations)['caption']\n",
        "\n",
        "        # return image with random annotation choice\n",
        "        return image, caption\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.img_keys[index]\n",
        "        return self.get_single_image(img_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKLgOMu3wukA",
        "outputId": "b5547e09-ed94-4170-b77f-a72ffbc491e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.70s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "# initialize image transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(args.side_length),              # resize images to same side_length\n",
        "    transforms.RandomCrop(args.side_length),          # Randomly crop image to side_length\n",
        "    transforms.RandomHorizontalFlip(),                # Random image flip horizontally\n",
        "    transforms.ToTensor(),                            # Convert image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])   # Normalize image tensor for resnet\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(args.side_length),              # Resize images to same side_length\n",
        "    transforms.CenterCrop(args.side_length),          # Center crop image to side_length\n",
        "    transforms.ToTensor(),                            # Convert image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])   # Normalize image tensor for resnet\n",
        "])\n",
        "\n",
        "# Initialize the full dataset\n",
        "full_dataset = IMAGE_DATASET(args.train_ann_path,\n",
        "                             args.train_img_data_path,\n",
        "                             train_transform)\n",
        "\n",
        "test_dataset = IMAGE_DATASET(args.val_ann_path,\n",
        "                             args.val_img_data_path,\n",
        "                             test_transform)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "# initialize the data loaders\n",
        "image_train_loader = DataLoader(dataset=train_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=4,\n",
        "                                pin_memory=True)\n",
        "image_val_loader = DataLoader(dataset=val_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=4,\n",
        "                                pin_memory=True)\n",
        "image_test_loader = DataLoader(dataset=test_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=4,\n",
        "                                pin_memory=True)\n",
        "\n",
        "# initialize vocabulary\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "counter = collections.Counter()\n",
        "\n",
        "for ann in list(full_dataset.coco.anns.values()) + list(test_dataset.coco.anns.values()):\n",
        "    caption = ann['caption'].strip()\n",
        "    tokens = tokenizer(caption)\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "    counter.update(tokens)\n",
        "\n",
        "sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "ordered_dict = collections.OrderedDict(sorted_by_freq)\n",
        "vocabulary = vocab(ordered_dict)\n",
        "vocabulary.append_token('<pad>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x72QCYiw0cj6"
      },
      "source": [
        "# Part 2: Neural Network Initialization\n",
        "- Two primary layers of the neural network are initialized. First a CNN layer that consists of a VGG-based model\n",
        "- An LSTM layer that uses the output of the CNN layer with 4096 features as a hidden input, the start token as the input, and passes each generated token as the input in the loop. The sentence is padded or cut at 20 tokens to ensure simplicity and consistency in data size during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-35z-S4xww4x"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # PART 0: PARAMS\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # PART 1: CNN\n",
        "        resnet =  models.resnet50(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.CNN = nn.Sequential(*modules)\n",
        "\n",
        "        self.additional_layers = nn.Sequential(\n",
        "          nn.PReLU(),\n",
        "          nn.Dropout(p=0.5),\n",
        "          nn.Linear(in_features=2048, out_features=args.embedding_dim)\n",
        "        )\n",
        "\n",
        "        self.FC_CNN = nn.Linear(args.embedding_dim, args.hidden_size)\n",
        "\n",
        "        # PART 2: LSTM\n",
        "        self.embedding = nn.Embedding(self.vocab_size, args.embedding_dim)\n",
        "        self.LSTM = nn.LSTM(args.embedding_dim, args.hidden_size, batch_first = True)\n",
        "        self.FC = nn.Linear(args.hidden_size, self.vocab_size)\n",
        "\n",
        "    def forward(self, x, captions = None):\n",
        "        with torch.no_grad():\n",
        "            cnn_features = self.CNN(x)\n",
        "\n",
        "        cnn_features = cnn_features.view(cnn_features.size(0), -1)\n",
        "        cnn_features = self.additional_layers(cnn_features)\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        h_i = torch.zeros(1, batch_size, args.hidden_size).to(args.device)\n",
        "        c_i = torch.zeros_like(h_i).to(args.device)\n",
        "\n",
        "        # INFERENCE MODE:\n",
        "        if captions is None:\n",
        "            out_logits = torch.zeros((batch_size, args.max_caption_length, self.vocab_size), device=args.device)\n",
        "            current_word = torch.zeros((batch_size, 1), dtype=torch.long, device=args.device)\n",
        "\n",
        "            for t in range(args.max_caption_length):\n",
        "                if t == 0:\n",
        "                    input_word = cnn_features.unsqueeze(1)\n",
        "                else:\n",
        "                    input_word = self.embedding(current_word)\n",
        "\n",
        "                output, (h_i, c_i) = self.LSTM(input_word, (h_i, c_i))\n",
        "\n",
        "                logits = self.FC(output.squeeze(1))\n",
        "                _, predicted_word = logits.max(1)\n",
        "\n",
        "                out_logits[:, t, :] = logits\n",
        "                current_word = predicted_word.unsqueeze(1)\n",
        "\n",
        "            return out_logits\n",
        "\n",
        "        # TRAINING MODE:\n",
        "        else:\n",
        "            embed = self.embedding(captions)\n",
        "            embed = torch.cat((cnn_features.unsqueeze(1), embed[:, :-1, :]), dim=1)\n",
        "            lstm_out, (h_i, c_i) = self.LSTM(embed, (h_i, c_i))\n",
        "            lstm_out = lstm_out\n",
        "            out_logits = self.FC(lstm_out)\n",
        "            return out_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HbIFPVxGwytj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e7e0ec-d1ed-4862-9f36-0b7daaca73c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 97.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# INITIALIZE NETWORK\n",
        "model = Model(len(vocabulary))\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g53WQoN30j6F"
      },
      "source": [
        "# Part 3: Training\n",
        "\n",
        "This part consists of the primary training loop over the desired number of epochs and specified batch size. The labels are tokenized, padded / shortened as needed, and the <sos> and <eos> tokens are added. The CNN output is then fed into the LSTM and a backpropagation is used based on the loss."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Using device {}'.format(device))\n",
        "\n",
        "# initialize tokenizer, loss, optimizer, scheduler\n",
        "tokenizer = tt.data.utils.get_tokenizer(\"basic_english\")\n",
        "loss_func = nn.CrossEntropyLoss(ignore_index = vocabulary['<pad>']).to(args.device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.1)"
      ],
      "metadata": {
        "id": "bRcXMSmTXvsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training helpers\n",
        "def pad_sequences(sequences, max_length, pad_token='<pad>'):\n",
        "    return [seq + [pad_token] * (max_length - len(seq)) if len(seq) < max_length else seq[:max_length] for seq in sequences]\n",
        "\n",
        "def remove_noise(sequences):\n",
        "    return [[token for token in seq if len(str(token)) > 1] for seq in sequences]\n",
        "\n",
        "def tokenize_and_pad(labels, vocab, max_length = args.max_caption_length):\n",
        "    tokenized_labels = [['<sos>'] + tokenizer(label) + ['<eos>'] for label in labels]\n",
        "    filtered_labels = remove_noise(tokenized_labels)\n",
        "    padded_labels = pad_sequences(filtered_labels, max_length)\n",
        "    indexed_labels = [[vocab[token] for token in seq] for seq in padded_labels]\n",
        "    return indexed_labels\n",
        "\n",
        "def process_tokens(predicted_tokens):\n",
        "    processed_strings = []\n",
        "    for tokens in predicted_tokens:\n",
        "        tokens = tokens[1:tokens.index(\"<eos>\")]\n",
        "        processed_string = ' '.join(tokens)\n",
        "        processed_strings.append(processed_string)\n",
        "\n",
        "    return processed_strings"
      ],
      "metadata": {
        "id": "0N3XFgZwXyTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxIAvYyBwznD"
      },
      "outputs": [],
      "source": [
        "# Main Training Loop\n",
        "def train(num_epochs, ckpt_load_idx = -1):\n",
        "    loss_history = []\n",
        "    start_epoch = 0\n",
        "\n",
        "    if ckpt_load_idx != -1:\n",
        "        ckpt_path = f'/content/drive/MyDrive/Image Caption Generator/checkpoint_{ckpt_load_idx}.pth'\n",
        "        checkpoint = torch.load(ckpt_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        loss_history = checkpoint['loss_history']\n",
        "\n",
        "    for idx_e in range(start_epoch, num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for idx, batch in enumerate(image_train_loader):\n",
        "            DATA, LABELS = batch\n",
        "\n",
        "            # Tokenize and pad labels\n",
        "            tokenized_labels = tokenize_and_pad(LABELS, vocabulary)\n",
        "\n",
        "            # Convert to tensor\n",
        "            target_sequences = torch.tensor(tokenized_labels, dtype=torch.long).to(device)\n",
        "\n",
        "            # Get LSTM predictions\n",
        "            pred = model(DATA.to(device))\n",
        "\n",
        "            pred_loss = pred.view(-1, pred.size(-1))\n",
        "\n",
        "            target_sequences_loss = target_sequences.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_func(pred_loss, target_sequences_loss)\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                # PRINT EPOCH + LOSS INFO\n",
        "                print(f'Epoch [{idx_e+1}/{num_epochs}], Step [{idx+1}/{len(image_train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "                # PRINT SAMPLE SENTENCE:\n",
        "                probabilities = torch.softmax(pred[0], dim=-1)\n",
        "                predicted_indices = torch.argmax(probabilities, dim=-1)\n",
        "                predicted_tokens = vocabulary.lookup_tokens(predicted_indices.tolist())\n",
        "                print(predicted_tokens, vocabulary.lookup_tokens(target_sequences[0].tolist()))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if idx_e != 0 and idx_e % 4 == 0:\n",
        "          ckpt_path = f'/content/drive/MyDrive/Image Caption Generator/checkpoint_{idx_e}.pth'\n",
        "          torch.save({\n",
        "              'epoch': idx_e,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss_history': loss_history\n",
        "          }, ckpt_path)\n",
        "\n",
        "\n",
        "    ckpt_path = f'/content/drive/MyDrive/Image Caption Generator/checkpoint_{idx_e}.pth'\n",
        "    torch.save({\n",
        "              'epoch': idx_e,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'scheduler_state_dict': scheduler.state_dict(),\n",
        "              'loss_history': loss_history\n",
        "    }, ckpt_path)\n",
        "\n",
        "train(args.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # most_common_words = counter.most_common(20)\n",
        "\n",
        "# # # Split the words and counts into separate lists\n",
        "# # words, counts = zip(*most_common_words)\n",
        "\n",
        "# # # Create the bar graph\n",
        "# # plt.figure(figsize=(10, 8))\n",
        "# # plt.bar(words, counts, color='blue')\n",
        "# # plt.xlabel('Words')\n",
        "# # plt.ylabel('Counts')\n",
        "# # plt.title('20 Most Common Words')\n",
        "# # plt.xticks(rotation=45)\n",
        "# # plt.show()\n",
        "\n",
        "# ckpt_path = f'/content/drive/MyDrive/Image Caption Generator/checkpoint_new.pth'\n",
        "# torch.save({\n",
        "#               'model_state_dict': model.state_dict(),\n",
        "#               'optimizer_state_dict': optimizer.state_dict(),\n",
        "#               'scheduler_state_dict': scheduler.state_dict(),\n",
        "# }, ckpt_path)"
      ],
      "metadata": {
        "id": "udM5zFUNa9BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkle3dDk0maw"
      },
      "source": [
        "# Part 4: Testing, Validation, and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Sampling: Visualize Samples from Full Dataset\n",
        "# model.eval()\n",
        "\n",
        "# # ckpt_path = '/content/drive/MyDrive/Image Caption Generator/checkpoint_test.pth'\n",
        "# # checkpoint = torch.load(ckpt_path)\n",
        "# # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# # # from torch.utils.data import Subset\n",
        "\n",
        "# random_indices = random.sample(range(len(full_dataset.img_keys)), 20)\n",
        "# subset = Subset(full_dataset, random_indices)\n",
        "# sample, true_labels = next(iter(DataLoader(subset, batch_size=20, shuffle=False)))\n",
        "\n",
        "# # # fig, axes = plt.subplots(1, 10, figsize=(15, 1.5))\n",
        "# # # for i, (img, label, ax) in enumerate(zip(images, true_labels, axes)):\n",
        "# # #     ax.imshow(img)\n",
        "# # #     ax.axis('off')\n",
        "# # #     ax.set_title(f\"Sample True Label: {label.item()}\\n Predicted Label {}\", fontsize=8)\n",
        "\n",
        "# # # plt.tight_layout()\n",
        "# # # plt.show()"
      ],
      "metadata": {
        "id": "Aqc0YDCM-cfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample = sample.to(args.device)"
      ],
      "metadata": {
        "id": "Qb7fPzfW6-9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# images = sample.to('cpu').permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "\n",
        "\n",
        "# # print(predicted_labels)\n",
        "\n",
        "# tokenized_labels = tokenize_and_pad(true_labels, vocabulary)\n",
        "\n",
        "#             # Convert to tensor\n",
        "# target_sequences = torch.tensor(tokenized_labels, dtype=torch.long).to(args.device)\n",
        "\n",
        "# predicted_labels = model(sample)\n",
        "\n",
        "# # print(predicted_labels)\n",
        "# # probabilities = torch.softmax(predicted_labels, dim=-1)\n",
        "# # predicted_indices = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "# predicted_tokens = [vocabulary.lookup_tokens(i.tolist()) for i in predicted_labels]\n",
        "# print(predicted_tokens)\n"
      ],
      "metadata": {
        "id": "J83yB9XrENGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, (img, label, pred) in enumerate(zip(images, true_labels, process_tokens(predicted_tokens))):\n",
        "#     fig, ax = plt.subplots(figsize=(1.5, 1.5))\n",
        "#     ax.imshow(img)\n",
        "#     ax.axis('off')\n",
        "#     ax.set_title(f\"Sample True Label: {label}\\nPredicted Label: {pred}\", fontsize=8)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "jzta4gQHFpa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9xHvPAe0rSw"
      },
      "source": [
        "# Part 5: Export Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image = Image.open('/content/drive/MyDrive/Image Caption Generator/data/IMG_7504.JPG').convert('RGB')\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((args.img_h, args.img_w)),  # Resize the image\n",
        "#     transforms.ToTensor()                         # Convert the image to a tensor\n",
        "# ])\n",
        "# image = transform(image).unsqueeze(0)\n",
        "\n",
        "# pred = lstm(image.to(device))\n",
        "# tokens = [vocabulary.lookup_tokens(i.tolist()) for i in pred]\n",
        "# label = process_tokens(tokens)\n",
        "\n",
        "# image = image.permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "# # for i, (img, label, pred) in enumerate(zip(images, true_labels, process_tokens(predicted_tokens))):\n",
        "# fig, ax = plt.subplots(figsize=(1.5, 1.5))\n",
        "# ax.imshow(image[0])\n",
        "# ax.axis('off')\n",
        "# ax.set_title(f\"Predicted Label: {label}\", fontsize=8)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "r8-1TzjiJZ1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4vdLmALupI7"
      },
      "outputs": [],
      "source": [
        "# files.download('/content/drive/MyDrive/Image Caption Generator/checkpoint_49.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}