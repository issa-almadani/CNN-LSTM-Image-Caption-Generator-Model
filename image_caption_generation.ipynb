{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Imports, Inializing Global Variables / Arguments, and Dataset and Vocabulary Initalization"
      ],
      "metadata": {
        "id": "m4GksisYujW-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ub1LqoKdw5O"
      },
      "outputs": [],
      "source": [
        "# 1 GOOGLE COLAB - DATASET IMPORTS\n",
        "import os\n",
        "import random\n",
        "\n",
        "# 2 BASIC TORCH IMPORTS\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 3 READING IN IMAGES FROM DATASET\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 4 TRANSFORMING IMAGES TO DATA\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 5 BUILDING VOCABULARY FROM CAPTIONS\n",
        "from torchvision.datasets import CocoCaptions\n",
        "import collections\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    # this class initializes global variables for use in the model and training\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'  # device to train on -> if GPU is availalble prioritizied\n",
        "\n",
        "        self.embedding_dim = 256      # embedding dimension for vocabulary\n",
        "        self.hidden_size = 1024       # hidden size for LSTM unit\n",
        "        self.max_caption_length = 20  # max caption length generated by model in training\n",
        "\n",
        "        self.batch_size = 128         # batch size for training\n",
        "        self.epochs = 5               # number of epochs to train\n",
        "        self.lr = 0.001               # ADAM optimizer LR\n",
        "        self.scheduler_rate = 2       # Scheduler decay rate\n",
        "        self.scheduler_gamma = 0.1    # Scheduler decays LR by this ratio\n",
        "\n",
        "        self.side_length = 256        # images resized to be this size\n",
        "\n",
        "        self.train_img_data_path = './train_data' # train dataset path\n",
        "        self.test_img_data_path = './test_data'   # test dataset path\n",
        "\n",
        "        self.train_ann_path = './captions_train2014.json' # train annotations path\n",
        "        self.test_ann_path = './captions_val2014.json'    # test annotations path\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "0leGja8YgMKM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block fetches the required dataset\n",
        "!wget -O train.zip http://images.cocodataset.org/zips/train2014.zip\n",
        "!wget -O test.zip http://images.cocodataset.org/zips/val2014.zip\n",
        "!wget -O annotations.zip http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "\n",
        "!mkdir -p train_data\n",
        "!mkdir -p test_data\n",
        "\n",
        "!unzip -oj train.zip -d train_data\n",
        "!unzip -oj test.zip -d test_data\n",
        "!unzip -oj annotations.zip -d ."
      ],
      "metadata": {
        "id": "8angPQB-eUy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMAGE_DATASET(Dataset):\n",
        "    # this class initializes the dataset loader interface to use with Pytorch\n",
        "    def __init__(self, annotations_file, img_dir, ids = None, transform = None, target_transform = None):\n",
        "        self.img_dir = img_dir                      # directory where dataset is stored\n",
        "        self.coco = COCO(annotations_file)          # annotation files for image dataset\n",
        "        self.transform = transform                  # transform functions to apply to image before it is returned\n",
        "        self.target_transform = target_transform    # transform functions to apply to target label before it is returned\n",
        "        self.img_keys = list(self.coco.imgs.keys()) if ids is None else ids # image IDs for all images in given dataset\n",
        "\n",
        "    def get_single_image(self, img_id):\n",
        "        # load image and annotations\n",
        "        img = self.coco.loadImgs(img_id)[0]\n",
        "        annotation_ids = self.coco.getAnnIds(img_id)\n",
        "        annotations = self.coco.loadAnns(annotation_ids)\n",
        "\n",
        "        # open and transform image to tensor with needed dimensions\n",
        "        path = os.path.join(self.img_dir, img['file_name'])\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "        # apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # choose a random caption out of all possible captions for image\n",
        "        caption = random.choice(annotations)['caption']\n",
        "\n",
        "        # return image with random caption choice\n",
        "        return image, caption\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # this method allows us to index the dataset like a list\n",
        "        img_id = self.img_keys[index]\n",
        "        return self.get_single_image(img_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_keys)"
      ],
      "metadata": {
        "id": "jpLCwuozfPx_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize image transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(args.side_length),              # resize images to same side_length\n",
        "    transforms.RandomCrop(args.side_length),          # Randomly crop image to side_length\n",
        "    transforms.RandomHorizontalFlip(),                # Random image flip horizontally\n",
        "    transforms.ToTensor(),                            # Convert image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])   # Normalize image tensor for resnet\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(args.side_length),              # Resize images to same side_length\n",
        "    transforms.CenterCrop(args.side_length),          # Center crop image to side_length\n",
        "    transforms.ToTensor(),                            # Convert image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])   # Normalize image tensor for resnet\n",
        "])\n",
        "\n",
        "# train - validation split / test dataset already seperate\n",
        "id_train, id_val = train_test_split(list(COCO(args.train_ann_path).imgs.keys()), test_size = 0.10)\n",
        "\n",
        "# Initialize the full dataset\n",
        "train_dataset = IMAGE_DATASET(args.train_ann_path,\n",
        "                             args.train_img_data_path,\n",
        "                             transform = train_transform,\n",
        "                             ids = id_train)\n",
        "\n",
        "validate_dataset = IMAGE_DATASET(args.train_ann_path,\n",
        "                             args.train_img_data_path,\n",
        "                             transform = test_transform,\n",
        "                             ids = id_val)\n",
        "\n",
        "test_dataset = IMAGE_DATASET(args.test_ann_path,\n",
        "                             args.test_img_data_path,\n",
        "                             transform = test_transform)\n",
        "\n",
        "# Initialize the dataloaders to be batched and randomized\n",
        "image_train_loader = DataLoader(dataset=train_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                shuffle=True,\n",
        "                                pin_memory=True)\n",
        "\n",
        "image_validate_loader = DataLoader(dataset=validate_dataset,\n",
        "                                  batch_size=args.batch_size,\n",
        "                                  shuffle=False,\n",
        "                                  pin_memory=True)\n",
        "\n",
        "image_test_loader = DataLoader(dataset=test_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                shuffle=False,\n",
        "                                pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug03azP0fieI",
        "outputId": "a7d81e7c-e7f1-4d75-dc20-676cb3bf4c24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.70s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.59s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.88s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.31s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This block loops over all possible captions to generate a vocabulary\n",
        "counter = Counter()\n",
        "\n",
        "# Tokenize captions in both train and test datasets\n",
        "for ann in list(train_dataset.coco.anns.values()) + list(test_dataset.coco.anns.values()):\n",
        "    caption = ann['caption'].strip()\n",
        "    tokens = word_tokenize(caption)\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "    counter.update(tokens)\n",
        "\n",
        "# Sort by frequency and create vocabulary\n",
        "sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "vocabulary = {'<pad>': 0, '<unk>': 1}\n",
        "\n",
        "# Assign indices to other tokens\n",
        "for idx, (token, _) in enumerate(sorted_by_freq, start=2):\n",
        "    vocabulary[token] = idx\n",
        "\n",
        "# Now you have a dictionary where tokens are mapped to indices\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Example: accessing the index for a specific token\n",
        "token_index = vocabulary.get('<sos>', None)\n",
        "\n",
        "# In case you want to create a reverse lookup from index to token\n",
        "index_to_token = {idx: token for token, idx in vocabulary.items()}"
      ],
      "metadata": {
        "id": "X1douYmx06ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block pre-trains a Word2Vec model to generate an embedding layer\n",
        "corpus = []\n",
        "for ann in list(train_dataset.coco.anns.values()) + list(test_dataset.coco.anns.values()):\n",
        "    caption = ann['caption'].strip()\n",
        "    tokens = word_tokenize(caption)\n",
        "    corpus.append(tokens)\n",
        "\n",
        "# Train Word2Vec\n",
        "embedding_model = Word2Vec(sentences = corpus, vector_size = args.embedding_dim, window = 5, min_count = 1, workers = 4)\n",
        "\n",
        "# Save the model if needed\n",
        "embedding_model.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "WoPr4zQdOONr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, args.embedding_dim))\n",
        "for word, idx in vocabulary.items():\n",
        "    if word in embedding_model.wv:\n",
        "        embedding_matrix[idx] = embedding_model.wv[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(size=(args.embedding_dim,))\n",
        "\n",
        "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float)"
      ],
      "metadata": {
        "id": "B8QDsfllPC-W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # PART 0: PARAMS\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # PART 1: CNN\n",
        "        resnet =  models.resnet50(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.CNN = nn.Sequential(*modules)\n",
        "\n",
        "        self.additional_layers = nn.Sequential(\n",
        "          nn.Dropout(p=0.5),\n",
        "          nn.Linear(in_features=2048, out_features=args.hidden_size),\n",
        "          nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.CNN_LSTM = nn.LSTM(args.hidden_size, args.hidden_size, batch_first = True, bidirectional = True)\n",
        "\n",
        "        # PART 2: EMBEDDING\n",
        "        self.embedding = nn.Embedding(vocab_size, args.embedding_dim, padding_idx = 0)\n",
        "        self.embedding.weight.data.copy_(embedding_tensor)\n",
        "\n",
        "        # PART 3: LSTM\n",
        "        self.LSTM = nn.LSTM(args.embedding_dim, args.hidden_size, batch_first = True, bidirectional = True)\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(args.hidden_size * 3, args.hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(args.hidden_size // 2, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, captions):\n",
        "        batch_size = x.size(0)\n",
        "        caption_length = captions.size(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            cnn_features = self.CNN(x).view(batch_size, -1)\n",
        "            cnn_features = self.additional_layers(cnn_features)\n",
        "            cnn_repeated = cnn_features.unsqueeze(1).repeat(1, batch_size, 1)\n",
        "            context, _ = self.CNN_LSTM(cnn_repeated)\n",
        "\n",
        "        embed = self.embedding(captions)\n",
        "        outputs, _ = self.LSTM(embed)\n",
        "\n",
        "        attention_scores = torch.bmm(context, outputs.transpose(1, 2))\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "        attention_context = torch.bmm(attention_scores, outputs)\n",
        "        context_vector = attention_context.sum(dim=1)\n",
        "\n",
        "        decoder_input = torch.cat((context_vector, cnn_features), dim=-1)\n",
        "        output = self.FC(decoder_input)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "tg9cGP3Nf9GR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZE NETWORK\n",
        "model = Model(len(vocabulary))\n",
        "model = model.to(args.device)\n",
        "\n",
        "print('Using device {}'.format(args.device))\n",
        "\n",
        "# initialize tokenizer, loss, optimizer, scheduler\n",
        "loss_func = nn.CrossEntropyLoss(ignore_index = vocabulary['<pad>']).to(args.device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.1)\n",
        "tloss_history = []\n",
        "vloss_history = []\n",
        "start_epoch = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_BkfENLgBFf",
        "outputId": "463ae850-45ad-4e87-947c-9775a6af983e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 148MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sequences, max_length, pad_token='<pad>'):\n",
        "    return [seq + [pad_token] * (max_length - len(seq)) if len(seq) < max_length else seq[:max_length] for seq in sequences]\n",
        "\n",
        "def tokenize_and_pad(labels, vocab, max_length = args.max_caption_length):\n",
        "    tokenized_labels = [['<sos>'] + word_tokenize(label.strip()) + ['<eos>'] for label in labels]\n",
        "    padded_labels = pad_sequences(tokenized_labels, max_length)\n",
        "    indexed_labels = [[vocab[token] for token in seq] for seq in padded_labels]\n",
        "    return indexed_labels\n",
        "\n",
        "def process_tokens(predicted_tokens):\n",
        "    processed_strings = []\n",
        "    for tokens in predicted_tokens:\n",
        "        tokens = tokens[1:tokens.index(\"\")]\n",
        "        processed_string = ' '.join(tokens)\n",
        "        processed_strings.append(processed_string)\n",
        "\n",
        "    return processed_strings"
      ],
      "metadata": {
        "id": "X9Pp2dqJgEpX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs):\n",
        "    best_vloss = float('inf')\n",
        "\n",
        "    for idx_e in range(start_epoch, num_epochs):\n",
        "        running_loss = 0.0\n",
        "        last_loss = 0.0\n",
        "        running_vloss = 0.0\n",
        "\n",
        "        model.train(True)\n",
        "\n",
        "        for idx, batch in enumerate(image_train_loader):\n",
        "            DATA, LABELS = batch\n",
        "\n",
        "            # Tokenize and pad labels\n",
        "            tokenized_labels = tokenize_and_pad(LABELS, vocabulary)\n",
        "\n",
        "            # Convert to tensor\n",
        "            target_sequences = torch.tensor(tokenized_labels, dtype=torch.long).to(args.device)\n",
        "\n",
        "            # Get LSTM predictions\n",
        "\n",
        "            predicted_captions = torch.zeros((target_sequences.size(0), args.max_caption_length, vocab_size), dtype=torch.float).to(args.device)\n",
        "            sos_one_hot = torch.nn.functional.one_hot(torch.tensor(vocabulary['<sos>']), num_classes=vocab_size).float().to(args.device)\n",
        "            predicted_captions[:, 0, :] = sos_one_hot\n",
        "\n",
        "            print(target_sequences)\n",
        "            return\n",
        "\n",
        "            for i in range(1, args.max_caption_length):\n",
        "                pred = model(DATA.to(args.device), target_sequences[:,:i])\n",
        "                predicted_captions[:, i, :] = pred\n",
        "\n",
        "            pred_loss = predicted_captions.view(-1, predicted_captions.size(-1))\n",
        "            target_sequences_loss = target_sequences.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_func(pred_loss, target_sequences_loss)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if idx % 50 == 49:\n",
        "                last_loss = running_loss / 50\n",
        "                tloss_history.append(last_loss)\n",
        "                running_loss = 0\n",
        "                print(f'Epoch [{idx_e+1}/{num_epochs}], Step [{idx+1}/{len(image_train_loader)}], Loss: {last_loss:.4f}')\n",
        "\n",
        "                # PRINT SAMPLE SENTENCE:\n",
        "                probabilities = torch.softmax(predicted_captions[0], dim=-1)\n",
        "                predicted_indices = torch.argmax(probabilities, dim=-1)\n",
        "                predicted_tokens = [index_to_token[index] for index in predicted_indices.tolist()]\n",
        "                print(predicted_tokens, [index_to_token[index] for index in target_sequences[0].tolist()])\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(image_validate_loader):\n",
        "                DATA, LABELS = batch\n",
        "                tokenized_labels = tokenize_and_pad(LABELS, vocabulary)\n",
        "                target_sequences = torch.tensor(tokenized_labels, dtype=torch.long).to(args.device)\n",
        "\n",
        "                predicted_captions = torch.zeros((target_sequences.size(0), args.max_caption_length, vocab_size), dtype=torch.float).to(args.device)\n",
        "                sos_one_hot = torch.nn.functional.one_hot(torch.tensor(vocabulary['<sos>']), num_classes=vocab_size).float().to(args.device)\n",
        "                predicted_captions[:, 0, :] = sos_one_hot\n",
        "\n",
        "                for i in range(1, args.max_caption_length):\n",
        "                    pred = model(DATA.to(args.device), torch.argmax(torch.softmax(predicted_captions[:,:i], dim=-1), dim=-1))\n",
        "                    predicted_captions[:, i, :] = pred\n",
        "\n",
        "                pred_loss = predicted_captions.view(-1, predicted_captions.size(-1))\n",
        "                target_sequences_loss = target_sequences.view(-1)\n",
        "                loss = loss_func(pred_loss, target_sequences_loss)\n",
        "                running_vloss += loss.item()\n",
        "\n",
        "        avg_vloss = running_vloss / len(image_validate_loader)\n",
        "        vloss_history.append(avg_vloss)\n",
        "\n",
        "        print(f'{idx_e + 1} LOSS train {last_loss} valid {avg_vloss}')\n",
        "\n",
        "        if avg_vloss < best_vloss:\n",
        "            best_vloss = avg_vloss\n",
        "            model_path = './model_{}'.format(idx_e + 1)\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        checkpoint = {\n",
        "          'start_epoch': idx_e + 1,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'tloss_history': tloss_history,\n",
        "          'vloss_history': vloss_history,\n",
        "          'vocabulary': vocabulary,\n",
        "          'index_to_token': index_to_token\n",
        "        }\n",
        "        torch.save(checkpoint, 'checkpoint.pth')\n",
        "        from google.colab import files\n",
        "        files.download('checkpoint.pth')\n",
        "        files.download('./model_{}'.format(idx_e + 1))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "# START TRAINING\n",
        "train(args.epochs)"
      ],
      "metadata": {
        "id": "f492h2-NMMV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "e9NlZbd5yJcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS BLOCK TO LOAD PRE-TRAINED MODEL\n",
        "\n",
        "checkpoint = torch.load('./checkpoint-final.pth')\n",
        "vocabulary = checkpoint[\"vocabulary\"]\n",
        "index_to_token = checkpoint[\"index_to_token\"]\n",
        "vocab_size = len(vocabulary)\n",
        "embedding_model = Word2Vec.load(\"word2vec-3.model\")\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "tloss_history = checkpoint['tloss_history']\n",
        "vloss_history = checkpoint['vloss_history']\n",
        "start_epoch = checkpoint['start_epoch']"
      ],
      "metadata": {
        "id": "Z2o0aCqJQMDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    corpus_score = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(image_test_loader):\n",
        "            DATA, LABELS = batch\n",
        "            tokenized_labels = tokenize_and_pad(LABELS, vocabulary)\n",
        "            target_sequences = torch.tensor(tokenized_labels, dtype=torch.long).to(args.device)\n",
        "\n",
        "            predicted_captions = torch.zeros((target_sequences.size(0), args.max_caption_length, vocab_size), dtype=torch.float).to(args.device)\n",
        "            sos_one_hot = torch.nn.functional.one_hot(torch.tensor(vocabulary['<sos>']), num_classes=vocab_size).float().to(args.device)\n",
        "            predicted_captions[:, 0, :] = sos_one_hot\n",
        "\n",
        "            for i in range(1, args.max_caption_length):\n",
        "                pred = model(DATA.to(args.device), torch.argmax(torch.softmax(predicted_captions[:,:i], dim=-1), dim=-1))\n",
        "                predicted_captions[:, i, :] = pred\n",
        "\n",
        "            pred_loss = predicted_captions.view(-1, predicted_captions.size(-1))\n",
        "            target_sequences_loss = target_sequences.view(-1)\n",
        "            loss = loss_func(pred_loss, target_sequences_loss)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            corpus_score += corpus_bleu([[index_to_token[index] for index in tokenized_labels[i]] for i in range(len(tokenized_labels))],\n",
        "                                        [[index_to_token[index] for index in predicted_captions.argmax(dim = -1).cpu().numpy()[i]] for i in range(len(DATA))])\n",
        "\n",
        "        test_loss /= len(image_test_loader)\n",
        "        corpus_score /= len(image_test_loader)\n",
        "\n",
        "    return test_loss, corpus_score\n",
        "\n",
        "test_loss, bleu_score = test()\n",
        "print(f'Test Loss: {test_loss}, Corpus Score: {bleu_score}')"
      ],
      "metadata": {
        "id": "G6_6Si65kPYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Images"
      ],
      "metadata": {
        "id": "cmBEIepsGTqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for i, batch in enumerate(image_test_loader):\n",
        "      DATA, LABELS = batch\n",
        "      tokenized_labels = tokenize_and_pad(LABELS, vocabulary)\n",
        "      target_sequences = torch.tensor(tokenized_labels, dtype=torch.long).to(args.device)\n",
        "\n",
        "      predicted_captions = torch.zeros((target_sequences.size(0), args.max_caption_length, vocab_size), dtype=torch.float).to(args.device)\n",
        "      sos_one_hot = torch.nn.functional.one_hot(torch.tensor(vocabulary['<sos>']), num_classes=vocab_size).float().to(args.device)\n",
        "      predicted_captions[:, 0, :] = sos_one_hot\n",
        "\n",
        "      for i in range(1, args.max_caption_length):\n",
        "          pred = model(DATA.to(args.device), torch.argmax(torch.softmax(predicted_captions[:,:i], dim=-1), dim=-1))\n",
        "          predicted_captions[:, i, :] = pred\n",
        "\n",
        "      break\n"
      ],
      "metadata": {
        "id": "yMGbP85jJP_8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.argmax(torch.softmax(predicted_captions[:,:i], dim=-1), dim=-1)"
      ],
      "metadata": {
        "id": "9rGfOKjtJhnA"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(DATA[6].numpy().transpose(1, 2, 0))\n",
        "plt.axis('off')\n",
        "plt.title([index_to_token[index] for index in predictions[6].cpu().numpy()], fontsize = 12, pad = 10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z6W6OcjNGST6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}